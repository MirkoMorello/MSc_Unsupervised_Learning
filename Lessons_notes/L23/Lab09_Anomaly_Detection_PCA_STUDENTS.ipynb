{"cells":[{"cell_type":"markdown","metadata":{"id":"KGQh867e_Xhb"},"source":["# **Lab experience #09 (STUDENTS): Anomaly Detection using proximity-based approaches vs PCA**\n","\n","This nineth lab session aims **to compare anomaly detection based on proximity-based approaches vs reconstruction-based methods (i.e., PCA)**. This lab session refers to all Prof. Stella's lectures on \"Introduction to anomaly detection\", \"Nearest neighbor based anomaly detection\", \"Clustering Based, Statistical Approaches and Reconstruction Based\".\n","\n","In this lab session, you willÂ **re-use code already developed in the previous labs** and better explore [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n","\n","The main task is to idenfity outliers in _a completely new dataset_ using kNN, LOF, and PCA. Verify agreement or the degree of mismatch among the three different methods. Then, remove the outliers (choosing the best performing method, at your choice), and then discuss on the quality of the remaining dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ckn8MenDX8hN"},"outputs":[],"source":["# Useful packages that you might want to use\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.spatial.distance import pdist as pdist\n","from scipy.spatial.distance import squareform as sf\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer"]},{"cell_type":"markdown","metadata":{"id":"HOf2CmRoVOl8"},"source":["# **Step 1**: Data loading, visual inspection, and pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4g3ZhIe90Lde"},"outputs":[],"source":["# Load the dataset\n","X = np.load('Dataset_lab09.npy')\n","[N,M] = np.shape(X)"]},{"cell_type":"markdown","metadata":{"id":"198RxTMucEN8"},"source":["Here, you are free to explore and pre-process your dataset, based on your experience and the guess you might make on the specific dataset. Hint: visualize the dataset in 2D, compute the proximity matrix, ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFJ79129er-x"},"outputs":[],"source":["# Visualization of data and proximity matrix pre-/post-scaling\n","#"]},{"cell_type":"markdown","metadata":{"id":"7VhX5MU6u_9L"},"source":["# **Step 2**: Investigation on outliers - Proximity-based approaches\n","\n","\n","Useful references:\n","- [NN](https://scikit-learn.org/stable/modules/neighbors.html) see section 1.6.1.1. \"Finding the Nearest Neighbors\" and also check previous lab solutions (e.g., Lab07, Lab08).\n","\n","- [LOF](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor) [and also slide no.8 and [WIki](https://en.wikipedia.org/wiki/Local_outlier_factor)]. Here, you don't have to manually implement the algorithm, but only to correctly use the sklearn package to obtain LOF."]},{"cell_type":"markdown","metadata":{"id":"ispZsqPFinXw"},"source":["**2.1 Nearest neighbor (NN)**\n","- Choose the value of the design parameters.\n","- Apply the algorithm.\n","- Discover, count, and label the outliers.\n","- Remove the outliers from the dataset, obtaining a _clean_ dataset (X_clean_NN)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U01SE8dFjCZz"},"outputs":[],"source":["# Apply the algorithm\n","from sklearn.neighbors import NearestNeighbors as knn\n","neighborhood_order = ..\n","\n","# Find neighborhood\n","neighborhood_set   = knn( n_neighbors=neighborhood_order, algorithm='ball_tree').fit(YOURDATA)\n","distances, indices = neighborhood_set.kneighbors(YOURDATA)\n","\n","# compute distances from kth nearest neighbors and sort them\n","dk_sorted     = np.sort(distances[:,-1])\n","dk_sorted_ind = np.argsort(distances[:,-1])\n","\n","\n","# Identify the outliers as those points with too high distance from their own kth nearest neighbor. Hint: choose one of the possible solutions of Lab09:\n","# 1: use the knee point                                 --> knee_x, knee_y\n","# 2: decide a percentage of outliers a-priori (n%)      --> n [%]\n","# 3: use a threshold from the above plot, left panel    --> dk_th = k\n","#\n","#\n","#\n","#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OhxBeyMofm9C"},"outputs":[],"source":["# Verify the outlier detection, count and label the outliers\n","figKNN = plt.figure('kNN distance values', figsize=(8,5))\n","# your plot\n","# add axes labels, title, legend (if needed), grid\n","plt.show()\n","\n","# Label=-1 to outliers\n","KNN_labels = np.ones(N)\n","KNN_labels[OUTLIERS_INDECES] = -1\n","\n","# Count the no. of outliers\n","countKNN = ...\n","print(countKNN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tb2OtL6RfPvN"},"outputs":[],"source":["# Visualize the solution in a 2D scatterplot or PCA/tSNE plot (use BLACK for OUTLIERS, yellow for inliers)\n","#"]},{"cell_type":"markdown","metadata":{"id":"3FYpXlMsd-6t"},"source":["**2.2 Local Outlier Factor (LOF)**\n","\n","- Choose the value of the design parameters.\n","- Apply the algorithm.\n","- Discover, count, and label the outliers.\n","- Remove the outliers from the dataset, obtaining a _clean_ dataset (X_clean_LOF)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Px92KXuykE6t"},"outputs":[],"source":["# Apply the algorithm\n","from sklearn.neighbors import LocalOutlierFactor\n","\n","lof_model  = LocalOutlierFactor(n_neighbors  = ...,\n","                                algorithm='ball_tree',\n","                                metric=...,\n","                                p=...,\n","                                metric_params = None,\n","                                contamination = ...)\n","\n","LOF_labels = lof_model.fit_predict(YOURDATA)     # predict the labels (1 inlier, -1 outlier) of X according to LOF\n","LOF_values = lof_model.negative_outlier_factor_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCKTIXNRkCzv"},"outputs":[],"source":["# Verify the outlier detection, count and label the outliers\n","figLOF = plt.figure('LOF values', figsize=(8,5))\n","# your plot\n","# add axes labels, title, legend (if needed), grid\n","plt.show()\n","\n","# Count the no. of outliers\n","countLOF = ...\n","print(countLOF)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9J-MJHC5flhl"},"outputs":[],"source":["# Visualize the solution in a 2D scatterplot or PCA/tSNE plot (use BLACK for OUTLIERS, yellow for inliers)\n","#\n","#\n","#"]},{"cell_type":"markdown","metadata":{"id":"dZjNOXfBgg6P"},"source":["# **Step 3**: Investigation on outliers - Principal Component Analysis (PCA)\n","\n","**Steps:**\n","\n","- Decide on the number of principal components (q)\n","- Run PCA. You can use [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n","- Reconstruct the dataset with (only) q components\n","- Compute the reconstruction error (RE) for every data points (from the q-dimensional space to the N-dimensional space)\n","- Identify, count, and label the outliers.\n","- Remove the outliers from the dataset, obtaining a _clean_ dataset (X_clean_PCA).\n","\n","_Note: sklearn.decomposition.PCA implements linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD._\n","\n","```q ``` is the same as ```NCOMP```."]},{"cell_type":"markdown","metadata":{"id":"JmRxem6CgIB3"},"source":["You can identify the outliers as those points too deviating from the main PCs.\n","\n","You learnt **two possible methods** to implement this idea (see Prof. Stella's slides nn.13-15):\n","\n","1.   use a **threshold on the reconstruction errors** --> \"Objects with large reconstruction errors are anomalies\"\n","2.   use the **Chi-square distribution**  --> \"An observation is anomalous if, for a given significance level alpha, the sum of the squared values of its projection on the first q PCs is larger than X^2_q(alpha).\n","\n","\n","Here, you can try either one of the two."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3PnSCfFjl3C"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# Design parameters\n","NCOMP = ...    # number of components\n","\n","# Apply the algo\n","pca = PCA(n_components=NCOMP)\n","pca_result = pca.fit_transform(YOURDATA)\n","print('PCA: explained variation per principal component: {}'.format(pca.explained_variance_ratio_.round(2)))\n","\n","# Compute the reconstruction error for every data point\n","#\n","#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkNCPTj6lEza"},"outputs":[],"source":["# Identify the outliers as those points too deviating from the main PCs. Hint: see Prof. Stella's slides nn.13-15\n","# 1: use a threshold                  --> \"Objects with large reconstruction errors are anomalies\"\n","# 2: use the Chi-square distribution  --> \"An observation is anomalous if, for a given significance level alpha, the sum of the squared values of its projection on the first q PCs\n","#                                         (normalized over the corresponding eigenvalues) is greater that the value of the Chi-square distribution in alpha.\"\n","#\n","#\n","#\n","#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4WGRuvxkhOb"},"outputs":[],"source":["# Verify the outlier detection, count and label the outliers\n","figPCA = plt.figure('PCA decomposition', figsize=(8,5))\n","# your plot\n","# add axes labels, title, legend (if needed), grid\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40UEGkMVh00r"},"outputs":[],"source":["# Labelling\n","PCA_labels ="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7oEw8R_h5FJ"},"outputs":[],"source":["# Count the no. of outliers\n","countPCA = ...\n","print(countPCA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47qMgeBsh8Mg"},"outputs":[],"source":["# Visualize the solution in a 2D scatterplot or PCA/tSNE plot (use BLACK for OUTLIERS, yellow for inliers)\n","#\n","#\n","#"]},{"cell_type":"markdown","metadata":{"id":"azifZsKShdfi"},"source":["# **Step 4**: Compare the above solutions\n","Hint: verify agreement or mismatches among the different algorithms (e.g., visual inspection, Rand index on labels, ...).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TcOovUaSkz2q"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"QeabstkukzWS"},"source":["# **Step 5**: Validate your solutions using the TRUE labels\n","Hint: use visual inspection, Rand index on labels, confusion matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrD9cqGDk0X6"},"outputs":[],"source":["# Load TRUE labels\n","#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olBuf6-tlWJ6"},"outputs":[],"source":["# Supervised validation\n","#"]},{"cell_type":"markdown","metadata":{"id":"A4fbyRSCEPYG"},"source":["# _This it the end of Lab session #09_ â\n"]},{"cell_type":"markdown","metadata":{"id":"ObZBE-RZidHs"},"source":["## Utility functions\n","\n","Retrieve utility functions from previous labs, if needed."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPFt5LTBsZm2rXxrO0hyWp2","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}

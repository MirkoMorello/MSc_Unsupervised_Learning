{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L15 10/04/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection, 3 approaches: Clustering based, Statistical Approaches and Reconstruction Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Based:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages:\n",
    "- Unsupervised Algorithm\n",
    "- existing aclustering algorithms can be plugged in\n",
    "\n",
    "#### Drawbacks:\n",
    "- if the data object does not have a natural clustering or the clustering algorithm is not able to detect the natural clusters, the techniques may fail\n",
    "- computationally expensive\n",
    "- In high dimesional spaces, data is sparse and distances between any two data objects may become quite similar\n",
    "- Can be difficult to decide on clustering technique\n",
    "- Can be difficult to decide on number of clusters\n",
    "- Outliers can distort the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that normal based instances belong to large and dense clusters, while **anomalies** do not belong to any significant cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach is simple:\n",
    "- Cluster data objects into a finite number of clusters\n",
    "- analyze each data object with respect to its  closest cluster\n",
    "- anomalous data objects will:\n",
    "  - not fit into any clusters\n",
    "  - belong to smalle clusters\n",
    "  - are located in low density clusters\n",
    "  - are far from other data  objects within the same cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Based Local Outlier Factor (CBLOF)\n",
    "\n",
    "A data object is a cluster-based  outlier if it does not strongly belong to any cluster\n",
    "- For prototype-based clusters, a data objects is an outlier if it is not close enough to a cluster center\n",
    "- For density-based clusters, a data objects is an outlier if its density is too low\n",
    "- For graph-based clusters, a data objects is an outlier if it is not well connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Statistical Approaches:\n",
    "Probabilisti definition of an outlier: an outlier is a data object that has a low probability with respect to a probability distribution model of the data.  \n",
    "Usually, we assume a parametric model describing the distribution of the data, like the normal distribution.  \n",
    "So we basically apply a statistical test that depends on:\n",
    "- Data distribution\n",
    "- Parameters of distribution\n",
    "- Number of expected outliers (confidence limit)\n",
    "\n",
    "Obviously the issues are that we not know the distribution of a data set and we have to identify it, we don't know the number of attributes or evevn if the data is a mixture of distributions.\n",
    "\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grubbs's Test:\n",
    "It detects outliers in univariate data, it assumes data comes from a normal distribution.  \n",
    "Detects one outlier at a time, remove the outlier and then repeat\n",
    "- $H_0$: there is no outlier in data\n",
    "- $H_1$: there is at least one outlier in data\n",
    "\n",
    "Grubbs's test statistic:\n",
    "$$G = \\frac{max|X-\\overline{X}|}{s}$$\n",
    "Reject $H_0$ if:\n",
    "$$G > \\frac{N-1}{\\sqrt{n}} \\sqrt{\\frac{t^2_{\\frac{\\alpha}{2N}, N-2}}{N-2+t^2_{\\frac{\\alpha}{2N}, N-2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood Approach\n",
    "Assumes the data set $D$ contains samples from a mixture of two probability distributions:\n",
    "- $M$ (majority/non-anomalous distribution)\n",
    "- $A$ (anomalous distribution)\n",
    "  \n",
    "#### General Approach:\n",
    "- Initially, assumes all the data objects belong to $M$\n",
    "- let $LL_t(D)$ be the log likelihood of $D$ at time $t$\n",
    "- for each data object $x_t$ taaht belongs to $M$, move it to $A$\n",
    "- Let $LL_{t+1}(D)$ be the new log likelihood\n",
    "- Computes the difference, $\\Delta = LL_t(D) - LL_{t+1}(D)$\n",
    "- if $\\Delta > c$ (which is some threshold), then $x_t$ is delcared as an anomaly and moved permanently from $M$ to $A$  \n",
    "\n",
    "Strengths and weaknesses:\n",
    "- Firm Mathematical foundation\n",
    "- Can be very efficient\n",
    "- Good results if distribution is known\n",
    "- In many cases, data distribution may not be known\n",
    "- For high Dimensional data, it may be difficult to estimate the  true distribution\n",
    "- Anomalies can distort the parameters of the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Reconstruction Based\n",
    "\n",
    "Let $x$ be the original data object, find the r epresentation of the data object in a lower dimensional space and project the object bato to the original space, call this object $\\widehat{x}$\n",
    "$$\\text{Reconstruction error} = ||x-\\widehat{x}||$$\n",
    "Objects with large reconstruction errors are anomalies  \n",
    "\n",
    "#### Principal Component Analysis\n",
    "Idea:\n",
    "- Compute the PCA of the dataset\n",
    "- For each test data object, compute its projection on these components\n",
    "- if $y_i$ denotes the $i^{th}$ component, then the following has chi-squared distribution $\\sum_{i=1}^q \\frac{y^{2}_i}{\\lambda_i}= \\frac{y^{2}_1}{\\lambda_1} + \\frac{y^{2}_2}{\\lambda_2}...+ \\frac{y^{2}_q}{\\lambda_q}$    for    $q<n$\n",
    "- A data object is anomalous if for a given significance level $\\alpha$, $\\sum_{i=1}^q \\frac{y^{2}_i}{\\lambda_i} > \\chi^2_q(\\alpha)$\n",
    "- Another measure is to observe the last few principal components, $\\sum_{i=p-r+1}^q \\frac{y^{2}_i}{\\lambda_i}$, anomalies have high value for the above quantity\n",
    "\n",
    "#### Autoencoders\n",
    "The idea is the same as the PCA, but autoencoders are a multi-layers NN, the number of input and output neurons is equeal to the numder of the original attributes  \n",
    "\n",
    "Strength and Weaknesses:\n",
    "- Does not require assumptions about distribution of normal class\n",
    "- Can use many dimensionality rediction approches\n",
    "- The reconstruction error is computer in the original space, this can be a problem if dimensionality is high"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
